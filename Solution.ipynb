{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69db90c5-35a0-4c7f-b1b0-c1e670c51d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2436b67a-9292-4f4c-9385-575d2aed21ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Data Loading ---\n",
    "# Load all the provided datasets.\n",
    "# It's crucial to handle potential errors during file loading.\n",
    "data_folder = 'assets'\n",
    "try:\n",
    "    print(\"Loading data...\")\n",
    "    train_df = pd.read_parquet(f'{data_folder}/train_data.parquet')\n",
    "    test_df = pd.read_parquet(f'{data_folder}/test_data.parquet')\n",
    "    add_trans_df = pd.read_parquet(f'{data_folder}/add_trans.parquet')\n",
    "    add_event_df = pd.read_parquet(f'{data_folder}/add_event.parquet')\n",
    "    offer_metadata_df = pd.read_parquet(f'{data_folder}/offer_metadata.parquet')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}. Make sure all parquet files are in the same directory.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd90d1c9-986b-47f4-9b17-f6c5f1cab5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering...\n",
      "Feature engineering completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 2. Feature Engineering ---\n",
    "# This is a critical step. We'll create new features to improve model accuracy.\n",
    "print(\"Starting feature engineering...\")\n",
    "\n",
    "# Ensure consistent data types for merging\n",
    "offer_metadata_df['id3'] = offer_metadata_df['id3'].astype(str)\n",
    "\n",
    "# Merge additional data into the main training and testing dataframes.\n",
    "train_df = train_df.merge(offer_metadata_df, on='id3', how='left')\n",
    "test_df = test_df.merge(offer_metadata_df, on='id3', how='left')\n",
    "\n",
    "# Ensure consistent data types for 'id2' column\n",
    "train_df['id2'] = train_df['id2'].astype(str)\n",
    "test_df['id2'] = test_df['id2'].astype(str)\n",
    "add_trans_df['id2'] = add_trans_df['id2'].astype(str)\n",
    "\n",
    "# Feature engineering on transaction data\n",
    "# Calculate aggregate transaction features for each customer.\n",
    "trans_agg = add_trans_df.groupby('id2').agg(\n",
    "    trans_count=('f367', 'count'),\n",
    "    trans_amount_sum=('f367', 'sum'),\n",
    "    trans_amount_mean=('f367', 'mean'),\n",
    "    trans_amount_std=('f367', 'std')\n",
    ").reset_index()\n",
    "\n",
    "train_df = train_df.merge(trans_agg, on='id2', how='left')\n",
    "test_df = test_df.merge(trans_agg, on='id2', how='left')\n",
    "\n",
    "# Ensure consistent data types for 'id2' in event data\n",
    "add_event_df['id2'] = add_event_df['id2'].astype(str)\n",
    "\n",
    "# Feature engineering on event data\n",
    "# Count the number of events for each customer.\n",
    "event_agg = add_event_df.groupby('id2').agg(\n",
    "    event_count=('id6', 'count')\n",
    ").reset_index()\n",
    "\n",
    "train_df = train_df.merge(event_agg, on='id2', how='left')\n",
    "test_df = test_df.merge(event_agg, on='id2', how='left')\n",
    "\n",
    "# Convert timestamp columns to datetime objects\n",
    "train_df['id4'] = pd.to_datetime(train_df['id4'])\n",
    "test_df['id4'] = pd.to_datetime(test_df['id4'])\n",
    "train_df['id12'] = pd.to_datetime(train_df['id12'])\n",
    "test_df['id12'] = pd.to_datetime(test_df['id12'])\n",
    "train_df['id13'] = pd.to_datetime(train_df['id13'])\n",
    "test_df['id13'] = pd.to_datetime(test_df['id13'])\n",
    "\n",
    "\n",
    "# Create time-based features\n",
    "train_df['day_of_week'] = train_df['id4'].dt.dayofweek\n",
    "train_df['hour_of_day'] = train_df['id4'].dt.hour\n",
    "test_df['day_of_week'] = test_df['id4'].dt.dayofweek\n",
    "test_df['hour_of_day'] = test_df['id4'].dt.hour\n",
    "\n",
    "# Offer duration\n",
    "train_df['offer_duration'] = (train_df['id13'] - train_df['id12']).dt.days\n",
    "test_df['offer_duration'] = (test_df['id13'] - test_df['id12']).dt.days\n",
    "print(\"Feature engineering completed.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "521a3334-371b-474a-94d0-32a50a6a63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 3. Data Preprocessing ---\n",
    "print(\"Preprocessing data...\")\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "# Exclude identifier columns and the target variable 'y'.\n",
    "features = [col for col in train_df.columns if col.startswith('f')]\n",
    "categorical_features = [col for col in features if train_df[col].dtype == 'object']\n",
    "numerical_features = [col for col in features if train_df[col].dtype != 'object']\n",
    "\n",
    "\n",
    "# Fill missing values.\n",
    "# For numerical features, we'll use the median.\n",
    "# For categorical features, we'll use the mode.\n",
    "for col in numerical_features:\n",
    "    median_val = train_df[col].median()\n",
    "    train_df[col] = train_df[col].fillna(median_val)\n",
    "    test_df[col] = test_df[col].fillna(median_val)\n",
    "\n",
    "for col in categorical_features:\n",
    "    mode_series = train_df[col].mode()\n",
    "    if len(mode_series) > 0:  # Check if mode exists\n",
    "        mode_val = mode_series[0]\n",
    "    else:\n",
    "        # If no mode exists (all values are unique), use the first non-null value or a default\n",
    "        non_null_vals = train_df[col].dropna()\n",
    "        mode_val = non_null_vals.iloc[0] if len(non_null_vals) > 0 else 'Unknown'\n",
    "    \n",
    "    train_df[col] = train_df[col].fillna(mode_val)\n",
    "    test_df[col] = test_df[col].fillna(mode_val)\n",
    "\n",
    "# Encode categorical features using Label Encoding with handling for unseen categories\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Fit on training data\n",
    "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
    "    \n",
    "    # Transform test data, using a special value for unseen categories\n",
    "    test_series = test_df[col].astype(str)\n",
    "    mask = test_series.isin(le.classes_)\n",
    "    \n",
    "    # Initialize with a value that will be out of range for the encoder\n",
    "    test_encoded = np.full(len(test_series), -1)\n",
    "    \n",
    "    # Only transform values that were seen during training\n",
    "    test_encoded[mask] = le.transform(test_series[mask])\n",
    "    \n",
    "    # Assign back to the dataframe\n",
    "    test_df[col] = test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f3ecdb-437d-4057-a3b0-4db5962a3bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the LightGBM model...\n",
      "Splitting data into training and validation sets...\n",
      "Training with early stopping...\n",
      "Feature count: 371\n",
      "Training samples: 616,131, Validation samples: 154,033\n",
      "[LightGBM] [Info] Number of positive: 29641, number of negative: 586490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.619471 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 47032\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 302\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.984997\n",
      "[LightGBM] [Info] Start training from score -2.984997\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[10]\tvalid_0's auc: 0.911601\n",
      "[20]\tvalid_0's auc: 0.918946\n",
      "[30]\tvalid_0's auc: 0.925099\n",
      "[40]\tvalid_0's auc: 0.928195\n",
      "[50]\tvalid_0's auc: 0.931999\n",
      "[60]\tvalid_0's auc: 0.933917\n",
      "[70]\tvalid_0's auc: 0.935218\n",
      "[80]\tvalid_0's auc: 0.936299\n",
      "[90]\tvalid_0's auc: 0.937394\n",
      "[100]\tvalid_0's auc: 0.9384\n",
      "[110]\tvalid_0's auc: 0.939213\n",
      "[120]\tvalid_0's auc: 0.939913\n",
      "[130]\tvalid_0's auc: 0.940501\n",
      "[140]\tvalid_0's auc: 0.941231\n",
      "[150]\tvalid_0's auc: 0.9416\n",
      "[160]\tvalid_0's auc: 0.942129\n",
      "[170]\tvalid_0's auc: 0.942643\n",
      "[180]\tvalid_0's auc: 0.943099\n",
      "[190]\tvalid_0's auc: 0.943392\n",
      "[200]\tvalid_0's auc: 0.943872\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[200]\tvalid_0's auc: 0.943872\n",
      "\n",
      "Top 20 most important features:\n",
      "    feature  importance\n",
      "367    f366          84\n",
      "351    f350          77\n",
      "133    f132          47\n",
      "207    f206          41\n",
      "204    f203          36\n",
      "32      f31          35\n",
      "205    f204          35\n",
      "139    f138          35\n",
      "211    f210          35\n",
      "352    f351          34\n",
      "39      f38          33\n",
      "364    f363          33\n",
      "313    f312          33\n",
      "69      f68          32\n",
      "208    f207          31\n",
      "315    f314          31\n",
      "47      f46          31\n",
      "131    f130          30\n",
      "135    f134          26\n",
      "148    f147          25\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4. Model Training ---\n",
    "print(\"Training the LightGBM model...\")\n",
    "\n",
    "# Define features (X) and target (y).\n",
    "# We will use all available features after preprocessing.\n",
    "all_features = numerical_features + categorical_features\n",
    "X = train_df[all_features]\n",
    "y = train_df['y']\n",
    "X_test = test_df[all_features]\n",
    "\n",
    "# LightGBM model parameters - optimized for faster training\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 200,  # Reduced for faster training\n",
    "    'learning_rate': 0.1,  # Increased for faster convergence\n",
    "    'num_leaves': 15,  # Reduced to prevent overfitting and speed up\n",
    "    'max_depth': 5,  # Limited depth for faster training\n",
    "    'min_child_samples': 20,  # Prevents overfitting on small leaves\n",
    "    'reg_alpha': 0.1,  # L1 regularization\n",
    "    'reg_lambda': 0.1,  # L2 regularization\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,  # Use all cores\n",
    "    'verbose': 1,  # Show progress\n",
    "    'colsample_bytree': 0.8,\n",
    "    'subsample': 0.8,\n",
    "    'subsample_freq': 5,  # Frequency for bagging\n",
    "}\n",
    "\n",
    "print(\"Splitting data into training and validation sets...\")\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Training with early stopping...\")\n",
    "print(\"Feature count:\", len(all_features))\n",
    "print(f\"Training samples: {len(X_train):,}, Validation samples: {len(X_val):,}\")\n",
    "\n",
    "# Initialize and train the model with callbacks\n",
    "model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "# Callbacks for better control\n",
    "callbacks = [\n",
    "    lgb.early_stopping(stopping_rounds=50, verbose=True),  # Stop if no improvement for 50 rounds\n",
    "    lgb.log_evaluation(period=10),  # Print progress every 10 rounds\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Print feature importance\n",
    "print(\"\\nTop 20 most important features:\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': all_features,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(feature_importance.head(20).to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c8877d-204c-4d85-ba81-6f734769ed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions and submission file...\n",
      "Template file not found. Creating submission with extracted IDs.\n",
      "Submission file 'r2_submission_YourTeamName.csv' created successfully!\n",
      "Good luck in the competition!\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Prediction and Submission File Generation ---\n",
    "print(\"Generating predictions and submission file...\")\n",
    "\n",
    "# Predict probabilities on the test set.\n",
    "predictions = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Create the submission dataframe with all required columns\n",
    "submission_df = pd.DataFrame({\n",
    "    'id1': test_df['id1'],\n",
    "    'id2': test_df['id1'].str.split('_').str[1].astype(int),  # Extract id2 from id1\n",
    "    'id3': (pd.to_datetime(test_df['id1'].str.split('_').str[3] + ' ' + \n",
    "                           test_df['id1'].str.split('_').str[4], \n",
    "                          format='%Y-%m-%d %H:%M:%S.%f')\n",
    "            .dt.strftime('%-m/%-d/%Y')),  # Format date as M/D/YYYY\n",
    "    'id5': '',  # This will be filled from the template if available\n",
    "    'pred': predictions\n",
    "})\n",
    "\n",
    "# Try to match with the template to ensure correct order and additional columns\n",
    "try:\n",
    "    template_df = pd.read_csv('685404e30cfdb_submission_template.csv')\n",
    "    # Keep only the prediction column from our model\n",
    "    submission_df = template_df[['id1', 'id2', 'id3', 'id5']].merge(\n",
    "        submission_df[['id1', 'pred']], \n",
    "        on='id1', \n",
    "        how='left'\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\"Template file not found. Creating submission with extracted IDs.\")\n",
    "\n",
    "\n",
    "# Save the submission file.\n",
    "# Replace `<team-name>` with your actual team name.\n",
    "team_name = \"YourTeamName\"  # IMPORTANT: Change this to your team name (no spaces, use underscores)\n",
    "submission_filename = f'r2_submission_{team_name}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file '{submission_filename}' created successfully!\")\n",
    "print(\"Good luck in the competition!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2759d996",
   "metadata": {},
   "source": [
    "# the end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
